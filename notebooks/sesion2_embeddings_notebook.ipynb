{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sesi√≥n 2: De Palabras a Embeddings\n",
    "\n",
    "**Curso:** Introducci√≥n a Large Language Models  \n",
    "**Profesor:** Francisco P√©rez Carbajal  \n",
    "**Basado en:** Hands-On Large Language Models, Chapter 1  \n",
    "\n",
    "---\n",
    "\n",
    "## üìö Contenido\n",
    "\n",
    "1. **Bag-of-Words**: El enfoque cl√°sico\n",
    "2. **Word2Vec**: Embeddings que capturan significado\n",
    "3. **Explorando similitudes**\n",
    "4. **Analog√≠as y operaciones**\n",
    "5. **Visualizaci√≥n en 2D**\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "- ‚úÖ Entender las limitaciones de bag-of-words\n",
    "- ‚úÖ Aprender qu√© son los embeddings\n",
    "- ‚úÖ Usar word2vec pre-entrenado\n",
    "- ‚úÖ Calcular similitudes sem√°nticas\n",
    "- ‚úÖ Visualizar embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Parte 1: Bag-of-Words\n",
    "\n",
    "## El Problema\n",
    "\n",
    "Las computadoras solo entienden n√∫meros. ¬øC√≥mo representamos texto?\n",
    "\n",
    "**Soluci√≥n cl√°sica:** Contar palabras ‚Üí \"Bag of Words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar dependencias\n",
    "!pip install scikit-learn gensim matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Corpus de ejemplo\n",
    "textos = [\n",
    "    \"El gato come pescado\",\n",
    "    \"El perro come carne\",\n",
    "    \"El gato y el perro son amigos\"\n",
    "]\n",
    "\n",
    "print(\"üìù Documentos originales:\")\n",
    "for i, texto in enumerate(textos, 1):\n",
    "    print(f\"  {i}. {texto}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear Representaci√≥n Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear vectorizador\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Transformar textos a vectores\n",
    "X = vectorizer.fit_transform(textos)\n",
    "\n",
    "# Obtener vocabulario\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"\\nüìñ Vocabulario ({len(vocab)} palabras):\")\n",
    "print(vocab)\n",
    "\n",
    "print(\"\\nüî¢ Vectores (Bag-of-Words):\")\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizaci√≥n en Tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataFrame para visualizaci√≥n\n",
    "df = pd.DataFrame(\n",
    "    X.toarray(),\n",
    "    columns=vocab,\n",
    "    index=[f\"Doc {i}\" for i in range(1, len(textos)+1)]\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Tabla de Bag-of-Words:\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Tu Turno: Agrega tu Oraci√≥n\n",
    "\n",
    "Modifica la lista `textos` arriba y vuelve a ejecutar las celdas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ùå Limitaciones de Bag-of-Words\n",
    "\n",
    "### Problema 1: Ignora el Orden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dos oraciones con diferente significado\n",
    "oraciones = [\n",
    "    \"El gato persigue al rat√≥n\",\n",
    "    \"El rat√≥n persigue al gato\"  # Significado diferente!\n",
    "]\n",
    "\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(oraciones)\n",
    "\n",
    "print(\"Oraci√≥n 1:\", oraciones[0])\n",
    "print(\"Oraci√≥n 2:\", oraciones[1])\n",
    "print(\"\\n¬øMisma representaci√≥n?\")\n",
    "print(np.array_equal(X[0].toarray(), X[1].toarray()))  # True!\n",
    "\n",
    "print(\"\\nVectores:\")\n",
    "print(X.toarray())\n",
    "print(\"\\n‚ùå ¬°Bag-of-words no puede distinguirlos!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problema 2: No Captura Significado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palabras con significado similar\n",
    "oraciones_sinonimos = [\n",
    "    \"Vi un gato\",\n",
    "    \"Vi un felino\"  # Mismo significado\n",
    "]\n",
    "\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(oraciones_sinonimos)\n",
    "\n",
    "print(\"Vocabulario:\", vec.get_feature_names_out())\n",
    "print(\"\\nVectores:\")\n",
    "print(X.toarray())\n",
    "print(\"\\n‚ùå 'gato' y 'felino' son completamente diferentes!\")\n",
    "print(\"   (aunque significan lo mismo)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problema 3: Alta Dimensionalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simular un vocabulario grande\n",
    "textos_largos = [\n",
    "    \"El gato come pescado fresco del mar\",\n",
    "    \"Los perros juegan en el parque grande\",\n",
    "    \"Los p√°jaros cantan en los √°rboles altos\",\n",
    "    \"El sol brilla sobre las monta√±as nevadas\"\n",
    "]\n",
    "\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(textos_largos)\n",
    "\n",
    "print(f\"Vocabulario: {len(vec.get_feature_names_out())} palabras\")\n",
    "print(f\"Tama√±o de vector por documento: {X.shape[1]} dimensiones\")\n",
    "print(f\"\\nSparsity (porcentaje de ceros): {100 * (1 - X.nnz / (X.shape[0] * X.shape[1])):.1f}%\")\n",
    "print(\"\\n‚ùå La mayor√≠a son ceros ‚Üí ineficiente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Parte 2: Word2Vec - Embeddings\n",
    "\n",
    "## üí° La Soluci√≥n\n",
    "\n",
    "En lugar de **contar**, **aprendemos** representaciones densas que capturan significado.\n",
    "\n",
    "**Word2Vec (2013):** \"Conocer√°s una palabra por las compa√±√≠as que tiene\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar Modelo Pre-entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Cargar modelo (puede tardar ~1 minuto la primera vez)\n",
    "print(\"Descargando word2vec...\")\n",
    "print(\"(Esto toma ~1 min la primera vez)\\n\")\n",
    "\n",
    "model = api.load('word2vec-google-news-300')\n",
    "\n",
    "print(\"‚úì Modelo cargado\")\n",
    "print(f\"‚úì Vocabulario: {len(model):,} palabras\")\n",
    "print(f\"‚úì Dimensi√≥n de embeddings: {model.vector_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorar un Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener embedding de \"cat\"\n",
    "palabra = \"cat\"\n",
    "embedding = model[palabra]\n",
    "\n",
    "print(f\"Embedding de '{palabra}':\")\n",
    "print(f\"  Dimensiones: {len(embedding)}\")\n",
    "print(f\"  Primeros 10 valores: {embedding[:10]}\")\n",
    "print(f\"  Tipo: vector denso (todos valores diferentes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Parte 3: Similitud Sem√°ntica\n",
    "\n",
    "## Palabras M√°s Similares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar palabras similares a \"cat\"\n",
    "palabra = \"cat\"\n",
    "similares = model.most_similar(palabra, topn=10)\n",
    "\n",
    "print(f\"üîç Palabras m√°s similares a '{palabra}':\")\n",
    "print(\"\\nPalabra         Similitud\")\n",
    "print(\"-\" * 30)\n",
    "for palabra_sim, score in similares:\n",
    "    print(f\"{palabra_sim:15s} {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Tu Turno: Explora Palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba con diferentes palabras\n",
    "palabras_explorar = [\"king\", \"computer\", \"happy\", \"soccer\"]\n",
    "\n",
    "for palabra in palabras_explorar:\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Similares a '{palabra}':\")\n",
    "    print(f\"{'='*40}\")\n",
    "    similares = model.most_similar(palabra, topn=5)\n",
    "    for pal, score in similares:\n",
    "        print(f\"  {pal:15s} {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrega tu propia palabra aqu√≠\n",
    "mi_palabra = \"\"  # Cambia esto\n",
    "\n",
    "if mi_palabra:\n",
    "    try:\n",
    "        similares = model.most_similar(mi_palabra, topn=5)\n",
    "        print(f\"Similares a '{mi_palabra}':\")\n",
    "        for pal, score in similares:\n",
    "            print(f\"  {pal}: {score:.4f}\")\n",
    "    except KeyError:\n",
    "        print(f\"‚ùå '{mi_palabra}' no est√° en el vocabulario\")\n",
    "        print(\"   Prueba con otra palabra en ingl√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similitud Entre Pares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar similitudes\n",
    "pares = [\n",
    "    (\"cat\", \"feline\"),    # Sin√≥nimos\n",
    "    (\"cat\", \"dog\"),       # Animales relacionados\n",
    "    (\"cat\", \"car\"),       # No relacionados\n",
    "    (\"king\", \"queen\"),    # Relacionados\n",
    "    (\"man\", \"woman\"),     # Relacionados\n",
    "]\n",
    "\n",
    "print(\"üìä Similitud entre pares:\\n\")\n",
    "print(f\"{'Par':<25} Similitud\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for palabra1, palabra2 in pares:\n",
    "    similitud = model.similarity(palabra1, palabra2)\n",
    "    par_str = f\"{palabra1} ‚Üî {palabra2}\"\n",
    "    print(f\"{par_str:<25} {similitud:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Parte 4: Analog√≠as\n",
    "\n",
    "## Operaciones Famosas\n",
    "\n",
    "**Idea:** Rey - Hombre + Mujer ‚âà Reina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analog√≠a cl√°sica: King - Man + Woman = Queen\n",
    "resultado = model.most_similar(\n",
    "    positive=['woman', 'king'],\n",
    "    negative=['man'],\n",
    "    topn=5\n",
    ")\n",
    "\n",
    "print(\"üëë King - Man + Woman = ?\\n\")\n",
    "for palabra, score in resultado:\n",
    "    print(f\"  {palabra:15s} {score:.4f}\")\n",
    "\n",
    "print(\"\\n‚úì ¬°El resultado es 'queen'!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√°s ejemplos de analog√≠as\n",
    "analogias = [\n",
    "    {\n",
    "        'nombre': 'Pa√≠ses y Capitales',\n",
    "        'ecuacion': 'Paris - France + Germany = ?',\n",
    "        'positive': ['Paris', 'Germany'],\n",
    "        'negative': ['France']\n",
    "    },\n",
    "    {\n",
    "        'nombre': 'Masculino/Femenino',\n",
    "        'ecuacion': 'actor - man + woman = ?',\n",
    "        'positive': ['actor', 'woman'],\n",
    "        'negative': ['man']\n",
    "    },\n",
    "    {\n",
    "        'nombre': 'Verbos',\n",
    "        'ecuacion': 'walking - walk + swim = ?',\n",
    "        'positive': ['walking', 'swim'],\n",
    "        'negative': ['walk']\n",
    "    }\n",
    "]\n",
    "\n",
    "for analogia in analogias:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"üß™ {analogia['nombre']}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Ecuaci√≥n: {analogia['ecuacion']}\\n\")\n",
    "    \n",
    "    resultado = model.most_similar(\n",
    "        positive=analogia['positive'],\n",
    "        negative=analogia['negative'],\n",
    "        topn=3\n",
    "    )\n",
    "    \n",
    "    for palabra, score in resultado:\n",
    "        print(f\"  {palabra:15s} {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Tu Turno: Crea tu Analog√≠a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dise√±a tu propia analog√≠a\n",
    "# Formato: A - B + C = ?\n",
    "\n",
    "mi_analogia = model.most_similar(\n",
    "    positive=['', ''],  # A y C\n",
    "    negative=[''],      # B\n",
    "    topn=5\n",
    ")\n",
    "\n",
    "# Descomenta y completa:\n",
    "# print(\"Mi analog√≠a:\")\n",
    "# for palabra, score in mi_analogia:\n",
    "#     print(f\"  {palabra}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Parte 5: Visualizaci√≥n en 2D\n",
    "\n",
    "## Reducir Dimensiones\n",
    "\n",
    "Embeddings tienen 300 dimensiones. Vamos a reducirlos a 2D para visualizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seleccionar palabras para visualizar\n",
    "palabras_animales = ['cat', 'dog', 'lion', 'tiger', 'elephant', 'mouse']\n",
    "palabras_vehiculos = ['car', 'bus', 'truck', 'bicycle', 'motorcycle', 'train']\n",
    "palabras = palabras_animales + palabras_vehiculos\n",
    "\n",
    "# Obtener embeddings\n",
    "embeddings = np.array([model[palabra] for palabra in palabras])\n",
    "\n",
    "print(f\"Embeddings originales: {embeddings.shape}\")\n",
    "print(\"Reduciendo a 2D con t-SNE...\")\n",
    "\n",
    "# Reducir dimensiones (300 ‚Üí 2)\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=5)\n",
    "embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "print(f\"Embeddings reducidos: {embeddings_2d.shape}\")\n",
    "print(\"‚úì Listo para visualizar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear visualizaci√≥n\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Separar por categor√≠a\n",
    "x_animales = embeddings_2d[:len(palabras_animales), 0]\n",
    "y_animales = embeddings_2d[:len(palabras_animales), 1]\n",
    "\n",
    "x_vehiculos = embeddings_2d[len(palabras_animales):, 0]\n",
    "y_vehiculos = embeddings_2d[len(palabras_animales):, 1]\n",
    "\n",
    "# Graficar\n",
    "plt.scatter(x_animales, y_animales, c='red', s=100, alpha=0.6, label='Animales')\n",
    "plt.scatter(x_vehiculos, y_vehiculos, c='blue', s=100, alpha=0.6, label='Veh√≠culos')\n",
    "\n",
    "# A√±adir etiquetas\n",
    "for i, palabra in enumerate(palabras):\n",
    "    plt.annotate(\n",
    "        palabra,\n",
    "        xy=(embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "        xytext=(5, 5),\n",
    "        textcoords='offset points',\n",
    "        fontsize=12,\n",
    "        fontweight='bold'\n",
    "    )\n",
    "\n",
    "plt.title('Visualizaci√≥n de Embeddings en 2D', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Dimensi√≥n 1', fontsize=12)\n",
    "plt.ylabel('Dimensi√≥n 2', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Observa c√≥mo:\")\n",
    "print(\"   ‚Ä¢ Animales est√°n agrupados (cluster rojo)\")\n",
    "print(\"   ‚Ä¢ Veh√≠culos est√°n agrupados (cluster azul)\")\n",
    "print(\"   ‚Ä¢ Palabras similares est√°n cerca en el espacio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Tu Turno: Visualiza tus Palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elige tus propias palabras para visualizar\n",
    "mis_palabras = [\n",
    "    'king', 'queen', 'prince', 'princess',  # Realeza\n",
    "    'apple', 'banana', 'orange', 'grape'    # Frutas\n",
    "]\n",
    "\n",
    "# Obtener embeddings\n",
    "mis_embeddings = np.array([model[w] for w in mis_palabras])\n",
    "\n",
    "# Reducir a 2D\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=3)\n",
    "mis_embeddings_2d = tsne.fit_transform(mis_embeddings)\n",
    "\n",
    "# Graficar\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(mis_embeddings_2d[:, 0], mis_embeddings_2d[:, 1], s=150, alpha=0.6)\n",
    "\n",
    "for i, palabra in enumerate(mis_palabras):\n",
    "    plt.annotate(\n",
    "        palabra,\n",
    "        xy=(mis_embeddings_2d[i, 0], mis_embeddings_2d[i, 1]),\n",
    "        fontsize=12\n",
    "    )\n",
    "\n",
    "plt.title('Mis Palabras en 2D')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Resumen y Reflexi√≥n\n",
    "\n",
    "## ‚úÖ Lo que Aprendimos\n",
    "\n",
    "### 1. Bag-of-Words\n",
    "- ‚úì Representa texto contando palabras\n",
    "- ‚úó Ignora orden\n",
    "- ‚úó No captura significado\n",
    "- ‚úó Alta dimensionalidad (sparse)\n",
    "\n",
    "### 2. Embeddings (Word2Vec)\n",
    "- ‚úì Vectores densos (300D)\n",
    "- ‚úì Capturan significado sem√°ntico\n",
    "- ‚úì Palabras similares ‚Üí vectores cercanos\n",
    "- ‚úì Permiten operaciones (analog√≠as)\n",
    "\n",
    "### 3. Aplicaciones\n",
    "- Similitud sem√°ntica\n",
    "- B√∫squeda de sin√≥nimos\n",
    "- Clustering de palabras\n",
    "- Base para LLMs modernos\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Insights Clave\n",
    "\n",
    "1. **Representaci√≥n importa:** C√≥mo representamos el lenguaje afecta todo lo dem√°s\n",
    "\n",
    "2. **Significado en contexto:** Word2Vec aprende significado viendo vecinos\n",
    "\n",
    "3. **Geometr√≠a sem√°ntica:** Similitud = distancia en espacio vectorial\n",
    "\n",
    "4. **Fundamento de LLMs:** Todos los modelos modernos usan embeddings\n",
    "\n",
    "---\n",
    "\n",
    "## üîÆ Pr√≥xima Sesi√≥n\n",
    "\n",
    "**Sesi√≥n 3:** Tipos de Embeddings\n",
    "\n",
    "- Sentence embeddings (frases completas)\n",
    "- Document embeddings (documentos)\n",
    "- Modelos contextuales (BERT)\n",
    "- Aplicaciones pr√°cticas\n",
    "\n",
    "---\n",
    "\n",
    "## üè† Para Explorar (Opcional)\n",
    "\n",
    "1. Prueba m√°s palabras y analog√≠as\n",
    "2. Visualiza diferentes categor√≠as sem√°nticas\n",
    "3. Piensa: ¬øQu√© aplicaci√≥n podr√≠as hacer con similitud sem√°ntica?\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Referencias\n",
    "\n",
    "- **Libro:** Hands-On Large Language Models (Chapter 1)\n",
    "- **Paper:** Mikolov et al. (2013) - \"Efficient Estimation of Word Representations\"\n",
    "- **Biblioteca:** Gensim - https://radimrehurek.com/gensim/\n",
    "- **Visualizaci√≥n:** Jay Alammar's blog - https://jalammar.github.io/\n",
    "\n",
    "**¬øDudas?** franciscop@ciencias.unam.mx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
